# Project Disaster Response Pipeline
This project is part of the Data Scientist Nanodegree Program.

### Table of Contents
1. [Project Motivation](#motivation)
2. [File Description](#files)
3. [Results](#results)
4. [Licensing, Authors](#licensing)

## Project Motivation <a name="motivation"></a>

The present project is a part of the Nano-degree program of Udacity. The task is to setup:
* an ETL Pipeline (Extract, transform, load) incl. cleaning + storing into SQLite db
* a machine learning Pipeline training a classifier
* a Flask Web App visualizing the results incl. flexible file path input possibility

The data the pipeline should be built on is about the messages sent out within a disaster situation (e.g. a hurricane).
The messages themselves are very precious to disaster responsibles (e.g. a responsible for water supply or fire fighter etc.) to efficiently organize help for the potential victims and people affected by the event. But the amount of messages and the unstructured nature of the data makes it difficult for the repsonsibles to grasp efficiently the most important for their area of responsibily. 
So in this project we're building as mentioned above a classifier classifying the messages into different categories to make it easier to filter the most important content for the responsibles. 

## File Description <a name="files"></a>
**app/**: Frontend for the visualization of the results </br>
**data/**: Data relevant for the given usecase and a script to preprocess the data. </br>
**models/**: Script to train the mentioned model within the pipeline and store it in a pkl file. </br>

## Results <a name="results"></a>
to be filled <br>

## Licensing, Authors <a name="licensing"></a>
The credit for the data goes to the company figure eight.